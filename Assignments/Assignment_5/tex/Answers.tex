\documentclass[
    headings=optiontohead,              % allows double headers
    12pt,                               % fontsize 
    DIV=13,                             % koma script diveider amount. tells koma how much of the site can be written to
    twoside=false,                      % if set to true, automatically formats as book style with different left and right pages
    open=right,                         % starting page on twosided texts 
    BCOR=00mm,                          % correction that accounts for the center of the pages being glued in
    toc=bibliographynumbered,            % bibliography gets a number and is listed in the table of contents
    parskip=half,                       % paragraph spacing
]{scrarticle}

\usepackage{fontspec}
\usepackage[english]{babel}                     % font that supports English
\usepackage{upgreek}                            % non-cursive Greek letters
\usepackage[stretch=10,shrink=10,protrusion=true,expansion=true,final]{microtype} % prettier block format
\usepackage{hyperref}                           % links for everything
\usepackage{color}                              % allows for setting in different colors
\usepackage[autooneside=false,automark]{scrlayer-scrpage} % page-style with "Kolumnentitel" (title of current chapter is displayed at the top)
\usepackage{amsfonts,amstext,amsmath,amsthm, amssymb} % better math mode (\mathrm and \text) and symbols
\usepackage[sb]{libertinus}                     % use the font libertinus (needs to be installed from the web)
%\usepackage[slantedGreek]{libertinust1math}     % math mode improvement for libertinus
\usepackage{siunitx}                            % physical units setting
\usepackage{icomma}                             % commas in lists get extra space if needed                        
\usepackage{xspace}                             % works to improve own commands and provides "\xspace"-command, that puts a space if needed
\usepackage{ifthen}                             % more control over non-obligatory parameters
\usepackage{titling}                            % get title values as macros
\usepackage[onehalfspacing]{setspace}           % control the spacing between lines and in enumeration lists
\usepackage[backend=biber, style=phys, biblabel=brackets, url=true]{biblatex} % citations with "modern" backend and an physics-accepted citation style
\usepackage{graphicx}                           % work with graphics 
\usepackage{ragged2e}                           % ragged-commands (when no block format is wanted)
\usepackage{pdfpages}                           % allows including of pdfs into this pdf
\usepackage{booktabs}                           % better table formatting
\usepackage{multicol}                           % allows for the definition of multi-columns in tables
\usepackage{multirow}                           % allows for the definition of multirow-tables instead of just multicolumn
\usepackage[section]{placeins}                  % provides the command "\FloatBarrier" to control the end of floatable regions for figures/tables
\usepackage{float}                              % provides the "H" option for forcing placement of a figure
\usepackage{floatpag}                           % make it possible for float-pages to not have a page number
\usepackage{url}                                % sometimes needed by biblatex, technically no longer needed
\usepackage{minted}                             % nice code highlighting (needs Python Package to compile!!)
\usepackage{mathtools}                          % more math control possibilities
\usepackage{eucal}                              % Different mathcal alphabet
\usepackage[autostyle=true]{csquotes}           % context-sensitive-quotes -> quotation marks that are set correctly for the context
\usepackage{physics}                            % bra-ket and more
\usepackage{nicematrix}                         % label row/cols on matrix
\usepackage{caption}                            % caption of different environments
\usepackage{subcaption}                         % subcaptions for figures
\usepackage{enumitem}                           % better enums
\usepackage{tikz}                               % Abbildungen zeichnen
    \usetikzlibrary{positioning}
\usepackage[toc, page]{appendix}
\setmonofont{Fira Code NF}
%\usepackage{parskip}

\input{../../../texconfig/config.tex}                                  % another file that holds the package/document configuration
\input{../../../texconfig/format.tex}                                  % another file that holds format information
\input{../../../texconfig/commands.tex}                                  % another file that holds format information

\title{Assignment Submission 5}
\author{Jan Claar}
\date{\today}
\fakultaeta{Multimedia Computing Lab}
\teachers{Prof. Dr. Rainer Lienhart, Katja Ludwig, Julian Lorenz}

\begin{document}
    \head 

    \exercise{5.1}

    When using convolutional layers, only the spatial height and width of the inputs are treated separately, while all other feature dimensions are combined into one output channel dimension. Consequently, the output must be reshaped to fit the dimensions of the label grid. Furthermore, since the model outputs a measure of likelyhood for each class (background, human) being contained in the corresponding bounding box, it has an additional dimension compared to the label grid, which only contains the class index of the actual class. 
    
    \exercise{5.3}

    Negative Mining is done to reduce the imbalance of positive to negative samples. Since most of the bounding boxes are negative samples, the model would mainly learn to predict negative samples with high confidence, which would reduce the loss, but has limited use for the actual task. To counteract this, the model is trained on a subset of the negative samples proportional to the number of positive samples, by multiplying the loss tensor with a mask that sets a random set of negative loss terms to zero. This should shift the focus of the training more on actually recognizing the positive samples. 

    To measure the model performance, a metric similar to the Intersection over Union is calculated. In this case, the number of correctly identified bounding boxes (intersection) is divided by the union of all positive targets and positive predictions. This is only done for the target class, as most of the bounding boxes are negative samples (background class), meaning a high metric for that class would not have any meaning.

    The model is trained for 50 epochs with a batch size of 16, an initial learning rate of 0.01 a momentum of 0.9 and a weight decay of 0.0005. Negative mining is done with a ratio of 2\,:\,1 negative to positive samples.

    The model with negative mining enabled achieves a quasi IoU of 0.01275 on the validation set, which is very low. However the model without negative mining achieves a quasi IoU of 0.0000, meaning even after 50 epochs of training it is not able to identify a single bounding box correctly. Interestingly, the overall loss of the regular model is much lower (\approx 0.008) than that of the model trained with negative mining (\approx 0.39). Since the loss is calculated over all bounding boxes during validation, it is not surprising that the regular model performs better in this regard, since it has seen much more of the overwhelmingly negative samples. \\
    The low overall score is likely due to suboptimal hyperparameters and no actual data augmentation, as well as the simple classifier module of the model, which consists only of an upsampling step and one convolutional layer. Nonetheless, negative mining proved to be effective in improving training results.

\end{document}
